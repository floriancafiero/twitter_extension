---
title: "Twitter_extension"
author: "Florian Cafiero"
date: "03/01/2021"
output: html_document
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



Penser à utiliser:
as_long_data_frame(graph)
net.df <- igraph::as_data_frame(...)

compg.df <- as.data.frame(list(Vertex=V(compg), Community=fccommunity, Hubscore=hubscore, Authscore=authscore), stringsAsFactors=FALSE)

## Notes

Regarder nombre de personnes qui RT chacun de nos acteurs ppx, communauté par communauté, et période par période.

Regardez les mentions - servent elles à pénétrer dans une nouvelle communauté ?

Différences producteurs de contenu et importants dans la communauté.

## Importation du réseau global

On isole ici les comptes Twitter ayant produit 5 tweets ou plus concernant la vaccination.

```{r import , results="hide", include=FALSE}
Sys.setlocale("LC_ALL", "fr_FR.UTF-8")
sup5_nodes <- read.csv("~/Desktop/Twitter/sup10_tweetos.csv", encoding="UTF-8", comment.char="#")
sup5_edges_glob <- read.csv("~/Desktop/Twitter/sup10_edges.csv", encoding="UTF-8", comment.char="#")
sup5_nodes <- sup5_nodes[,-1] #supprime la première colonne inutile
sup5_edges_glob <- sup5_edges_glob[,-1] #supprime la première colonne inutile
sup5_nodes <- sup5_nodes[,c(2,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32)] # On intervertit les deux premières colonnes, pour commencer par l'user_id
sup5_edges_glob <- sup5_edges_glob[,c(1,4,2,3,5,6,7)] # On se débrouille pour avoir source et target comme première et deuxième colonnes.
library(plyr)
library(dplyr)
sup5_edges <- filter(sup5_edges_glob, sup5_edges_glob[,5]!= "mention") #on filtre pour ne garder que les RT et commenté
sup5_edges <- filter(sup5_edges, sup5_edges[,5]!= "RT_comment") #on filtre pour ne garder que les RT
```

## Importation des propriétés des noeuds période par période 

```{r import nodes by period, results="hide", message=FALSE}
Sys.setlocale("LC_ALL", "fr_FR.UTF-8")
sup5_nodes_period1 <- read.csv("sup10_tweetos_periode1.csv", encoding="UTF-8", comment.char="#")
sup5_nodes_period2 <- read.csv("sup10_tweetos_periode2.csv", encoding="UTF-8", comment.char="#")
sup5_nodes_period3 <- read.csv("sup10_tweetos_periode3.csv", encoding="UTF-8", comment.char="#")
sup5_nodes_period4 <- read.csv("sup10_tweetos_periode4.csv", encoding="UTF-8", comment.char="#")
sup5_nodes_period5 <- read.csv("sup10_tweetos_periode5.csv", encoding="UTF-8", comment.char="#")
```

## Fusion avec BDD politique

We realize a left outer join, simply using the base R function, to merge political positioning indicators and our tweeters base.

```{r political, results="hide", message=FALSE}
political <-read.csv("~/Downloads/newest_engaged_account_unique_follower_sets_oordinates_2_3.csv")
sup5_nodes_political <- merge(sup5_nodes, political, by=c("user_id"), all.x = TRUE)
sup5_nodes_political_period1 <- merge(sup5_nodes_period1, political, by=c("user_id"), all.x = TRUE)
sup5_nodes_political_period2 <- merge(sup5_nodes_period2, political, by=c("user_id"), all.x = TRUE)
sup5_nodes_political_period3 <- merge(sup5_nodes_period3, political, by=c("user_id"), all.x = TRUE)
sup5_nodes_political_period4 <- merge(sup5_nodes_period4, political, by=c("user_id"), all.x = TRUE)
sup5_nodes_political_period5 <- merge(sup5_nodes_period5, political, by=c("user_id"), all.x = TRUE)
```

## Fusion avec codage quali

We realize a left outer join, simply using the base R function, to merge political positioning indicators and our tweeters base.

```{r qualit, results="hide", message=FALSE}
quali <-read.csv("~/Downloads/codage_quali_tweetos.csv", sep=";", comment.char="#")
bddfinale <- merge(sup5_nodes_political, quali, by=c("user_id"), all.x = TRUE)
bddfinale <- bddfinale[-c(38042, 44613,  44656, 45618, 45835,  46433, 46970, 46985, 47538, 47590, 47871, 48530, 49395, 49412,  49830,  50158, 50177,  50396,  50674, 50991, 51183, 51356, 51488, 51935, 52603, 52679, 52916, 53135, 53276,53493),]
names(bddfinale)[names(bddfinale) == "user"] <- "name"

n_occur <- data.frame(table(bddfinale$user_id))
n_occur[n_occur$Freq > 1,] #on vérifie l'absence de doublons
```

```{r qualit by period}
bddfinale_period1 <- merge(sup5_nodes_political_period1, quali, by=c("user_id"), all.x = TRUE)
n_occur_p1 <- data.frame(table(bddfinale_period1$user_id))
n_occur_p1[n_occur_p1$Freq > 1,]

bddfinale_period2 <- merge(sup5_nodes_political_period2, quali, by=c("user_id"), all.x = TRUE)
n_occur_p2 <- data.frame(table(bddfinale_period2$user_id))
n_occur_p2[n_occur_p2$Freq > 1,]

bddfinale_period3 <- merge(sup5_nodes_political_period3, quali, by=c("user_id"), all.x = TRUE)
n_occur_p3 <- data.frame(table(bddfinale_period3$user_id))
n_occur_p3[n_occur_p3$Freq > 1,]

bddfinale_period4 <- merge(sup5_nodes_political_period4, quali, by=c("user_id"), all.x = TRUE)
n_occur_p4 <- data.frame(table(bddfinale_period1$user_id))
n_occur_p4[n_occur_p4$Freq > 1,]

bddfinale_period5 <- merge(sup5_nodes_political_period5, quali, by=c("user_id"), all.x = TRUE)
n_occur_p5 <- data.frame(table(bddfinale_period5$user_id))
n_occur_p5[n_occur_p5$Freq > 1,]
```


## Création de périodes de temps et de bases de données associées

```{r period, message=FALSE}
library("dplyr")
sup5_edges <- filter(sup5_edges, sup5_edges[,1]!= "NA") #on filtre les sources des liens dont l'identité n'a pas pu être  correctement crawlée
sup5_edges <- filter(sup5_edges, sup5_edges[,2]!= "NA") #on filtre les destinations des liens dont l'identité n'a pas pu être  correctement crawlée
sup5_edges <- filter(sup5_edges, sup5_edges[,1]%in% bddfinale$user_id & sup5_edges[,2] %in% bddfinale$user_id)

sup5_edges$timeperiod <- as.Date(sup5_edges$time) #on pense à bien mettre la date au format date pour permettre les comparaisons, et on se permet de négliger l'heure
sup5_edges$timeperiod = sup5_edges$timeperiod = case_when(
    sup5_edges$timeperiod < "2016-06-15" ~ "avant_juin_2016",
    "2016-06-14" < sup5_edges$timeperiod & sup5_edges$timeperiod < "2016-12-15" ~ "2016_SEM2",
    "2016-12-14" < sup5_edges$timeperiod & sup5_edges$timeperiod < "2017-06-15" ~ "2017_SEM1",

    "2017-06-14" < sup5_edges$timeperiod & sup5_edges$timeperiod < "2017-12-15" ~ "2017_SEM2",
    "2017-12-14" < sup5_edges$timeperiod & sup5_edges$timeperiod < "2018-06-15" ~ "2018_SEM1",
    "2018-06-14" < sup5_edges$timeperiod & sup5_edges$timeperiod < "2018-12-15" ~ "2018_SEM2",
    "2018-12-14" < sup5_edges$timeperiod ~ "apres_janv_2019")
sup5_edges <- filter(sup5_edges, sup5_edges$timeperiod!="apres_janv_2019")
sup5_edges <- filter(sup5_edges, sup5_edges$timeperiod!="avant_juin_2016")
sup5_edges <- filter(sup5_edges, sup5_edges$timeperiod!="NA")
```

##Suprresion des isolats

```{r deleting isolates, message=FALSE}
library(igraph)
network_vax <- graph.data.frame(sup5_edges, directed=TRUE, vertices = bddfinale)
network_vax_noiso <- delete_vertices (network_vax, V(network_vax)[degree(network_vax)==0]) # on enlève les isolats, pour ne pas les prendre en compte dans la détection de communauté etc. 
```

## Détection de communautés - Louvain non pondéré

Pour la détection de communautés, on s'assure que l'on travaille avec le minimum d'informations requise. On retire d'abord tous les doublons potentiels:

```{r removing duplicates, message=FALSE}
library(dplyr)
sup5_edges_community <- sup5_edges[,-c(6,7,8)]
sup5_edges_community <- unique(sup5_edges_community)
network_vax_final <- graph.data.frame(sup5_edges_community, directed=TRUE, vertices = bddfinale)
network_vax_final <- delete_vertices (network_vax, V(network_vax)[degree(network_vax)==0])
```

On s'intéresse seulement au plus grands composants du réseaux, on élimine les plus petits composants (le second plus grand composant contient 12 noeuds, le 3ème noeuds, le 4ème 6 noeuds etc.).

```{r removing small components, results="hide", message=FALSE}
components_final <- components(graph = network_vax_final)
biggest_cluster_id <- which.max(components_final$csize)
vert_ids <- V(network_vax_final)[components_final$membership == biggest_cluster_id]
network_vax_final_component <- igraph::induced_subgraph(network_vax_final, vert_ids)
```

On peut ensuite lancer la détection de communautés sur le réseau global. On tente d'abord d'aplatir le réseau, sous forme de réseau non dirigé, et d'exécuter un algorithme de Louvain.

```{r undirected approximation and Louvain, results="hide", message=FALSE}
network_vax_undirected <- as.undirected(network_vax_final_component, mode = c("collapse"))
louvain_undirected <- cluster_louvain(network_vax_undirected)
sizes(louvain_undirected)
```

## Détection de communautés - Louvain pondéré

On peut choisir de pondérer les liens entre compte par le nombre de RT.

```{r finding weight, results="hide", message=FALSE}
weight <- ddply(sup5_edges,.(source_id,target_id), nrow)
weight$V1
sup5_edges_community <- left_join(sup5_edges_community, weight, by=c("source_id","target_id"))
```

On rééxecute alors l'algorithme de Louvain en intégrant au calcul le poids nouvellement créé.

```{r weighted Louvain, results="hide", message=FALSE}
network_vax_final_weighted <- graph.data.frame(sup5_edges_community, directed=TRUE, vertices = bddfinale)
network_vax_final_weighted <- delete_vertices (network_vax, V(network_vax)[degree(network_vax)==0])
components_final_weighted <- components(graph = network_vax_final_weighted)
biggest_cluster_id_w <- which.max(components_final_weighted$csize)
vert_ids_w <- V(network_vax_final_weighted)[components_final$membership == biggest_cluster_id_w]
network_vax_final_component_w <- igraph::induced_subgraph(network_vax_final_weighted, vert_ids_w)
network_vax_undirected_w <- as.undirected(network_vax_final_component_w, mode = c("collapse"))
louvain_undirected_w <- cluster_louvain(network_vax_undirected_w, weights= network_vax_undirected_w$V1)
sizes(louvain_undirected_w)
```

On ré-attache les communautés aux noeuds dans le réseau:

```{r communities to edges weighted Louvain, results="hide", message=FALSE}
network_vax_undirected_w$community <- louvain_undirected_w$membership
network_vax_final_component_w$community <- louvain_undirected_w$membership
```

On ré-attache les communautés aux noeuds dans la bdd des noeuds:

```{r communities to edges attachment, results="hide", message=FALSE}
tmp <- cbind(as.data.frame(louvain_undirected_w$names), as.data.frame(louvain_undirected_w$membership))
bddfinale$community <- NA
bddfinale$community <- tmp$`louvain_undirected_w$membership`[match(bddfinale$name,tmp$`louvain_undirected_w$names`)]
```

Et on fait de même pour les tables par période:

```{r communities to edges attachment period by period, results="hide", message=FALSE}
community <- as.data.frame(cbind(bddfinale$community, bddfinale$user_id))
names(community)[names(community) == "V1"] <- "community"
names(community)[names(community) == "V2"] <- "user_id"
colnames(community)
bddfinale_period1 <- merge(bddfinale_period1, community, by=c("user_id"), all.x = TRUE)
names(bddfinale_period1)[names(bddfinale_period1) == "community.y"] <- "community"
bddfinale_period1$community
bddfinale_period2 <- merge(bddfinale_period2, community, by=c("user_id"), all.x = TRUE)
names(bddfinale_period2)[names(bddfinale_period2) == "community.y"] <- "community"
bddfinale_period2$community
bddfinale_period3 <- merge(bddfinale_period3, community, by=c("user_id"), all.x = TRUE)
names(bddfinale_period3)[names(bddfinale_period3) == "community.y"] <- "community"
bddfinale_period3$community
bddfinale_period4 <- merge(bddfinale_period4, community, by=c("user_id"), all.x = TRUE)
names(bddfinale_period4)[names(bddfinale_period4) == "community.y"] <- "community"
bddfinale_period4$community
bddfinale_period5 <- merge(bddfinale_period5, community, by=c("user_id"), all.x = TRUE)
names(bddfinale_period5)[names(bddfinale_period5) == "community.y"] <- "community"
bddfinale_period5$community
```

## Analyse des communautés

```{r communities analysis, message=FALSE}
communities <- data.frame() 
for (i in unique(network_vax_final_component_w$community)) 
  { 
# create subgraphs for each community subgraph 
subgraph <- induced_subgraph(network_vax_final_component_w, v = which(network_vax_final_component_w$community == i)) 
# get size of each subgraph 
size <- igraph::gorder(subgraph) 
#sort by size
if (igraph::gorder(subgraph) > 100){
# get betweenness centrality 
btwn <- igraph::betweenness(subgraph, directed=TRUE) 
communities <- communities %>% 
  dplyr::bind_rows(data.frame(
    community= i, 
    n_members = size, 
    most_important = names(which(btwn == max(btwn))) 
    ) 
  ) 
} 
}
knitr::kable(
  communities %>% 
    dplyr::select(community, n_members, most_important)
)
```

On cherche les 5 comptes dont la centralité de degré est la plus élevée, pour chaque communauté importante détectée.

```{r degree centrality, message=FALSE}
top_five <- data.frame() 
for (i in unique(network_vax_final_component_w$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_vax_final_component_w, v = which(network_vax_final_component_w$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree <- igraph::degree(subgraph, mode = c("in")) 
    # get top five degrees 
    top <- names(head(sort(degree, decreasing = TRUE), 10))
    result <- data.frame(community = i, rank = 1:10, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL, username= NULL) 
  } 
  top_five <- top_five %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_five %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```


On recode les catégories

```{r recoding categories, message=FALSE}
louvain_undirected_w$membership <-recode(louvain_undirected_w$membership,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"   
)

bddfinale$community <- recode(bddfinale$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)

bddfinale_period1$community <- recode(bddfinale_period1$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)

bddfinale_period2$community <- recode(bddfinale_period2$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)

bddfinale_period3$community <- recode(bddfinale_period3$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)

bddfinale_period4$community <- recode(bddfinale_period4$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)


bddfinale_period5$community <- recode(bddfinale_period5$community,
  "23" = "proscience_med_france",
  "49" = "mainstream_media",
  "48" = "conspiracy_and_far_left",
  "31" = "public_health_france",
  "38" = "public_health_international",
  "20"= "conspiracy_and_far_right_france",
  "13" = "random",
  "24" = "pro_science_france",
  "13" = "random",
  "27" = "media_public_health_canada",
  "21" = "animal_health", 
  "44"=  "conspiracy_and_far_right_international",
  .default = "other"     
)

```

On ré-attache les communautés aux noeuds dans le réseau:

```{r attaching communities to edges, message=FALSE}
network_vax_undirected_w$community <- louvain_undirected_w$membership
network_vax_final_component_w$community <- louvain_undirected_w$membership
```

On ré-attache les communautés aux noeuds dans la bdd des noeuds:

```{r cbind communities, message=FALSE}
tmp <- cbind(as.data.frame(louvain_undirected_w$names), as.data.frame(louvain_undirected_w$membership))
bddfinale$community <- NA
bddfinale$community <- tmp$`louvain_undirected_w$membership`[match(bddfinale$name,tmp$`louvain_undirected_w$names`)]
```

## Analyse des bios

On sépare les bdd par communautés


```{r bdd by communities, include=FALSE}
bddproscience_med_fr <- bddfinale[which(bddfinale$community == "proscience_med_france" ),]
bddmedia_fr <- bddfinale[which(bddfinale$community == "mainstream_media" ),]
bdd_far_left <- bddfinale[which(bddfinale$community == "conspiracy_and_far_left"),]
bdd_p_health_fr <- bddfinale[which(bddfinale$community == "public_health_france"),]
bdd_p_health_canada <- bddfinale[which(bddfinale$community == "media_public_health_canada"),]
bdd_far_right_fr <- bddfinale[which(bddfinale$community== "conspiracy_and_far_right_france"),]
bdd_far_right_int <- bddfinale[which(bddfinale$community == "conspiracy_and_far_right_international"),]
bdd_random <- bddfinale[which(bddfinale$community == "random" ),]
bdd_proscience_fr <- bddfinale[which(bddfinale$community == "pro_science_france"),]
bdd_p_health_int <- bddfinale[which(bddfinale$community == "public_health_international"),]
bdd_animal <- bddfinale[which(bddfinale$community== "animal_health"),]
```

Puis on fusionne dans chaque série de bios entre elles, puis on les exporte en .txt pour traitement extérieur.

```{r bios by community - promed, include=FALSE}
library(rtweet)        # Used for extracting the tweets
library(tm)            # Text mining cleaning
library(stringr)       # Removing characters
library(qdapRegex)     # Removing URLs 
library(wordcloud2) 
bioproscience_med_fr <- paste(bddproscience_med_fr$bio)
# cleaning the text
bioproscience_med_fr <- 
  bioproscience_med_fr %>%
  str_remove("\\n") %>%                   # remove linebreaks
  rm_twitter_url() %>%                    # Remove URLS
  rm_url() %>%
  str_remove_all("#\\S+") %>%             # Remove any hashtags
  str_remove_all("@\\S+") %>%             # Remove any @ mentions
  removeWords(stopwords("french")) %>%   # Remove common words (a, the, it etc.)
  removeWords(stopwords("english")) %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(c("amp"))                   # Final cleanup of other small changes
write.table(bioproscience_med_fr, file = "bioprosciences_med.txt", sep = "\t",row.names = FALSE)

bioproscience_fr <- paste(bdd_proscience_fr$bio)
# cleaning the text
bioproscience_fr <- 
  bioproscience_fr %>%
  str_remove("\\n") %>%                   # remove linebreaks
  rm_twitter_url() %>%                    # Remove URLS
  rm_url() %>%
  str_remove_all("#\\S+") %>%             # Remove any hashtags
  str_remove_all("@\\S+") %>%             # Remove any @ mentions
  removeWords(stopwords("french")) %>%   # Remove common words (a, the, it etc.)
  removeWords(stopwords("english")) %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(c("amp"))                   # Final cleanup of other small changes
write.table(bioproscience_fr, file = "bioproscience.txt", sep = "\t",row.names = FALSE)


biofarright_fr <- paste(bdd_far_right_fr$bio)
# cleaning the text
biofarright_fr  <- 
  biofarright_fr  %>%
  str_remove("\\n") %>%                   # remove linebreaks
  rm_twitter_url() %>%                    # Remove URLS
  rm_url() %>%
  str_remove_all("#\\S+") %>%             # Remove any hashtags
  str_remove_all("@\\S+") %>%             # Remove any @ mentions
  removeWords(stopwords("french")) %>%   # Remove common words (a, the, it etc.)
  removeWords(stopwords("english")) %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(c("amp"))                   # Final cleanup of other small changes
write.table(biofarright_fr, file = "biofarright_fr.txt", sep = "\t",row.names = FALSE)

biofarleft <- paste(bdd_far_left$bio)
# cleaning the text
biofarleft  <- 
  biofarleft  %>%
  str_remove("\\n") %>%                   # remove linebreaks
  rm_twitter_url() %>%                    # Remove URLS
  rm_url() %>%
  str_remove_all("#\\S+") %>%             # Remove any hashtags
  str_remove_all("@\\S+") %>%             # Remove any @ mentions
  removeWords(stopwords("french")) %>%   # Remove common words (a, the, it etc.)
  removeWords(stopwords("english")) %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(c("amp"))                   # Final cleanup of other small changes
write.table(biofarleft, file = "biofarleft_fr.txt", sep = "\t",row.names = FALSE)


biomedia_fr <- paste(bddmedia_fr$bio)
# cleaning the text
biomedia_fr  <- 
  biomedia_fr  %>%
  str_remove("\\n") %>%                   # remove linebreaks
  rm_twitter_url() %>%                    # Remove URLS
  rm_url() %>%
  str_remove_all("#\\S+") %>%             # Remove any hashtags
  str_remove_all("@\\S+") %>%             # Remove any @ mentions
  removeWords(stopwords("french")) %>%   # Remove common words (a, the, it etc.)
  removeWords(stopwords("english")) %>%
  removeNumbers() %>%
  stripWhitespace() %>%
  removeWords(c("amp"))                   # Final cleanup of other small changes
write.table(biomedia_fr, file = "biomedia_fr.txt", sep = "\t",row.names = FALSE)



```


bdd_p_health_fr <- bddfinale[which(bddfinale$community_public_health_france> 0),]
bdd_p_health_canada <- bddfinale[which(bddfinale$community_media_public_health_canada> 0),]

bdd_far_right_int <- bddfinale[which(bddfinale$community_conspiracy_and_far_right_international> 0),]
bdd_far_right_int <- bddfinale[which(bddfinale$community_conspiracy_and_far_right_international> 0),]
bdd_random <- bddfinale[which(bddfinale$community_random> 0),]

bdd_p_health_int <- bddfinale[which(bddfinale$community_public_health_international> 0),]
bdd_animal <- bddfinale[which(bddfinale$community_animal_health> 0),]


## Analyse politique

```{r political period, message=FALSE}
boxplot(political$col0, bddfinale_period1$col0, bddfinale_period2$col0, bddfinale_period3$col0, bddfinale_period4$col0, bddfinale_period5$col0, main="attitude to institutions")

boxplot(political$col1, bddfinale_period1$col1, bddfinale_period2$col1, bddfinale_period3$col1, bddfinale_period4$col1, bddfinale_period5$col1, main="left / right ")
```

La population qui parle de vaccins sur Twitter semble répartie comme dans la population. Peu d'évolution au cours du temps en apparence. Si on se concentre sur les plus investis sur Twitter:

```{r political period active, message=FALSE}
bddfinale_period1_active <- bddfinale_period1[which(bddfinale_period1$v_tweets_periode > 500),]
bddfinale_period2_active <- bddfinale_period2[which(bddfinale_period2$v_tweets_periode > 500),]
bddfinale_period3_active <- bddfinale_period3[which(bddfinale_period3$v_tweets_periode > 500),]
bddfinale_period4_active <- bddfinale_period4[which(bddfinale_period4$v_tweets_periode > 500),]
bddfinale_period5_active <- bddfinale_period5[which(bddfinale_period5$v_tweets_periode > 500),]
boxplot(political$col0, bddfinale_period1_active$col0, bddfinale_period2_active$col0, bddfinale_period3_active$col0, bddfinale_period4_active$col0, bddfinale_period5_active$col0, main="attitude to institutions")
boxplot(political$col1, bddfinale_period1_active$col1, bddfinale_period2_active$col1, bddfinale_period3_active$col1, bddfinale_period4_active$col1, bddfinale_period5_active$col1, main="left / right ")
```

Si on regarde maintenant qui parle le plus de vaccins:

```{r political period active quantiles, message=FALSE}
quantile(bddfinale$n_tweets_BDD, probs = seq(.1, .95, by = .05))
```

Pour être dans les 5% les plus actifs, il faut en moyenne avoir publié ca. 10 messages sur les vaccins par période. 


```{r political period active about vaccines, message=FALSE}
bddfinale_period1_active_vax <- bddfinale_period1[which(bddfinale_period1$n_tweets_BDD > 10),]
bddfinale_period2_active_vax <- bddfinale_period2[which(bddfinale_period2$n_tweets_BDD > 10),]
bddfinale_period3_active_vax <- bddfinale_period3[which(bddfinale_period3$n_tweets_BDD > 10),]
bddfinale_period4_active_vax <- bddfinale_period4[which(bddfinale_period4$n_tweets_BDD > 10),]
bddfinale_period5_active_vax <- bddfinale_period5[which(bddfinale_period5$n_tweets_BDD > 10),]
boxplot(political$col0, bddfinale_period1_active_vax$col0, bddfinale_period2_active_vax$col0, bddfinale_period3_active_vax$col0, bddfinale_period4_active_vax$col0, bddfinale_period5_active_vax$col0, main="attitude to institutions")
boxplot(political$col1, bddfinale_period1_active_vax$col1, bddfinale_period2_active_vax$col1, bddfinale_period3_active_vax$col1, bddfinale_period4_active_vax$col1, bddfinale_period5_active_vax$col1, main="left / right ")
```


Quelle structure politique par communauté ?

```{r political by community, message=FALSE}
boxplot(bddfinale$col0 ~ bddfinale$community, xlab = "Community", ylab="Attitude towards institutions",  cex.axis= 0.7,col=terrain.colors(12), names=c("animal", "conspiracy FL", "conspiracy FR", "conspiracy INT", "media", "public health CA", "other", "proscience", "promed", "public health FR", "public health INT", "random"))

boxplot(bddfinale$col1 ~ bddfinale$community, xlab = "Community", ylab="Left / right ", cex.axis= 0.7, col=terrain.colors(12), names=c("animal", "conspiracy FL", "conspiracy FR", "conspiracy INT", "media", "public health CA", "other", "proscience", "promed", "public health FR", "public health INT", "random"))
```

Quelle politisation des individus les plus centraux ? Période par période ?


## Analyse des communautés détectées

### Checking

```{r communities checking, message=FALSE}

bddfinale_annote <- filter(bddfinale, bddfinale$PRO_ANTI_OUT!= "NA")
table_comm_pro_anti <- table (bddfinale_annote$PRO_ANTI_OUT, bddfinale_annote$community)
table_comm_pro_anti
```
On observe certains phénomènes intéressants. Tout d'abord, les classements par communauté semblent majoritairement recouvrir ceux de l'annotation. Quelques exceptions cependant: 13 critiques de la vaccination sont classés dans la communauté 11, celle des pro-science. Il s'agit en fait de "critiques", mais pas d'antivaccins, souvent issus du milieu médical (pharmacritique, docdu16, martin winckler, formindep etc) qui ne critiquent que certains aspects de la politique vaccinale (HPV, grippe notamment).

## Réseaux dirigés divisés par période

On crée des "subgraphs" pour chacune des périodes de six mois définies.  

```{r creating timperiods directed, message=FALSE}
network_p1 <- delete.edges(network_vax_final_component_w, which(E(network_vax_final_component_w)$timeperiod!="2016_SEM2"))
network_p2 <- delete.edges(network_vax_final_component_w, which(E(network_vax_final_component_w)$timeperiod !="2017_SEM1"))
network_p3 <- delete.edges(network_vax_final_component_w, which(E(network_vax_final_component_w)$timeperiod != "2017_SEM2"))
network_p4 <- delete.edges(network_vax_final_component_w, which(E(network_vax_final_component_w)$timeperiod !="2018_SEM1"))
network_p5 <- delete.edges(network_vax_final_component_w, which(E(network_vax_final_component_w)$timeperiod !="2018_SEM2"))
```

## Hubs et autorités

```{r hubs and authorities, message=FALSE}
hs <- hub_score(network_vax_final_component_w, weights=NA)$vector
as <- authority_score(network_vax_final_component_w, weights=NA)$vector
```


```{r hubs and authorities p1, message=FALSE}
hs_p1 <- hub_score(network_p1, weights=NA)$vector
hs_p1_top <- names(head(sort(hs_p1, decreasing = TRUE), 15)) 
write.csv(hs_p1_top, "hs_p1_top.csv")
as_p1 <- authority_score(network_p1, weights=NA)$vector
as_p1_top <- names(head(sort(as_p1, decreasing = TRUE), 15)) 
as_p1_top
write.csv(as_p1_top, "as_p1_top.csv")
```

```{r hubs and authorities p2, message=FALSE}
hs_p2 <- hub_score(network_p2, weights=NA)$vector
hs_p2_top <- names(head(sort(hs_p2, decreasing = TRUE), 15)) 
hs_p2_top
write.csv(hs_p2_top, "hs_p2_top.csv")
as_p2 <- authority_score(network_p2, weights=NA)$vector
as_p2_top <- names(head(sort(as_p2, decreasing = TRUE), 15))
as_p2_top
write.csv(as_p2_top, "as_p2_top.csv")
```

```{r hubs and authorities p3, message=FALSE}
hs_p3 <- hub_score(network_p3, weights=NA)$vector
hs_p3_top <- names(head(sort(hs_p3, decreasing = TRUE), 15))
hs_p3_top
write.csv(hs_p3_top, "hs_p3_top.csv")
as_p3 <- authority_score(network_p3, weights=NA)$vector
as_p3_top <- names(head(sort(as_p3, decreasing = TRUE), 15))
as_p3_top
write.csv(as_p3_top, "as_p3_top.csv")
```

```{r hubs and authorities p4, message=FALSE}
hs_p4 <- hub_score(network_p4, weights=NA)$vector
hs_p4_top <- names(head(sort(hs_p4, decreasing = TRUE), 15))
hs_p4_top
write.csv(hs_p4_top, "hs_p4_top.csv")
as_p4 <- authority_score(network_p4, weights=NA)$vector
as_p4_top <- names(head(sort(as_p4, decreasing = TRUE), 15)) 
as_p4_top
write.csv(as_p4_top, "as_p4_top.csv")
```

```{r hubs and authorities p5, message=FALSE}
hs_p5 <- hub_score(network_p5, weights=NA)$vector
hs_p5_top <- names(head(sort(hs_p5, decreasing = TRUE), 15)) 
hs_p5_top
write.csv(hs_p5_top, "hs_p5_top.csv")
as_p5 <- authority_score(network_p5, weights=NA)$vector
as_p5_top <- names(head(sort(as_p5, decreasing = TRUE), 15)) 
as_p5_top
write.csv(as_p5_top, "as_p5_top.csv")
```

## Cliques

Pas très pertinent pour réseau dirigé...

```{r cliques, message=FALSE}
cliques_max <- largest_cliques(network_vax_final_component_w)
cliques_max
```

Checker dupont-Aignan ; asselineau

### Variation de followers


```{r boxplot v_followers, message=FALSE}
boxplot(bddfinale$v_followers_periode ~ bddfinale$community, xlab= "Community", ylab="Variation of followers", col=terrain.colors(12), outline=FALSE,  cex.axis= 0.5, main="Variaton of followers during the whole period")
```
Peu de différence de variation de followers. Regarder peut être ceux qui en ont le plus gagné ?

Période par période:

```{r boxplot v_followers by period, message=FALSE}
boxplot(bddfinale_period1$v_followers_periode ~ bddfinale_period1$community, xlab= "Community", ylab="Variation of followers - period 1", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5, main="Variaton of followers during period 1")

boxplot(bddfinale_period2$v_followers_periode ~ bddfinale_period2$community, xlab= "Community", ylab="Variation of followers - period 2", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5, main="Variaton of followers during period 2")

boxplot(bddfinale_period3$v_followers_periode ~ bddfinale_period3$community, xlab= "Community", ylab="Variation of followers - period 3", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5, main="Variaton of followers during period 3")

boxplot(bddfinale_period4$v_followers_periode ~ bddfinale_period4$community, xlab= "Community", ylab="Variation of followers - period 4", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5, main="Variaton of followers during period 4")

boxplot(bddfinale_period5$v_followers_periode ~ bddfinale_period5$community, xlab= "Community", ylab="Variation of followers - period 5", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5, main="Variaton of followers during period 5")
```

On fait une régression, en prenant comme situation de référence la communauté "out", et en ajoutant comme variable l'appartanence à chaque communauté, l'activité (nb de tweets produits), le pourcentage de RT, et le pourcentage de RT via.

On recode pour cela les communautés principales en dummy, et on élimine les NA.

```{r recoding communities as dummies, message=FALSE}
library(fastDummies)
bddfinale <- dummy_cols(bddfinale, select_columns = 'community')
bddfinale_comm <- filter(bddfinale, bddfinale$community != "NA")
```

```{r boxplot tweet vax, message=FALSE}
boxplot(bddfinale$taux_tweets_vac ~ bddfinale$community, xlab= "Community", ylab="Percentage of tweets regarding vaccines", col=terrain.colors(12), outline=FALSE,  cex.axis= 0.5)
```

```{r boxplot tweet antivax, message=FALSE}
boxplot(bddfinale$taux_antivac ~ bddfinale$community, xlab= "Community", ylab="Percentage of tweets regarding anti-vaccine", col=terrain.colors(12), outline=FALSE, cex.axis= 0.5)
```

On réalise notre régression:

```{r followers variation, message=FALSE}
followers_reg <- lm(bddfinale$v_followers_periode ~ bddfinale$community_animal_health + bddfinale$community_public_health_france + bddfinale$community_media_public_health_canada + bddfinale$community_proscience_med_france + bddfinale$community_pro_science_france + bddfinale$community_conspiracy_and_far_right_france + bddfinale$community_conspiracy_and_far_right_international + bddfinale$community_conspiracy_and_far_left + bddfinale$community_mainstream_media + bddfinale$community_random)
summary(followers_reg)
```

```{r followers par tweet vac variation, message=FALSE}
followers_reg <- lm(bddfinale$v_followers_periode ~ bddfinale$taux_tweets_vac + bddfinale$n_tweets_BDD)
summary(followers_reg)
```


```{r discretisation tweets_antivac, message=FALSE}
library(fastDummies)
bddfinale$quartile_tx_tweets_vac <- cut(bddfinale$taux_tweets_vac, breaks=quantile(bddfinale$taux_tweets_vac, c(0, 0.25, 0.5, 0.75, 1), na.rm=TRUE), include.lowest = TRUE, labels=c("Q1","Q2","Q3","Q4"))
bddfinale$quartile_tx_twwets_vac
bddfinale <- dummy_cols(bddfinale, select_columns = 'quartile_tx_tweets_vac')
```

On réalise ensuite notre régression en contrôlant par des paramètres d'activité:

```{r followers variation controlled, message=FALSE}
library(jtools)
library(broom)
followers_reg_controlled <- lm(bddfinale$v_followers_periode ~ bddfinale$community_other + bddfinale$community_animal_health + bddfinale$community_public_health_france + bddfinale$community_media_public_health_canada + bddfinale$community_proscience_med_france + + bddfinale$community_pro_science_france +  bddfinale$community_conspiracy_and_far_right_france + bddfinale$community_conspiracy_and_far_right_international + bddfinale$community_conspiracy_and_far_left + bddfinale$taux_majuscules + bddfinale$taux_antivac + bddfinale$community_mainstream_media +  bddfinale$n_tweets_BDD + bddfinale$active_period + bddfinale$RTpercent + bddfinale$RT_via_percent + bddfinale$taux_tweets_vac + bddfinale$verified + bddfinale$N_mentions + bddfinale$n_followers)
summary(followers_reg_controlled)
plot_summs(followers_reg_controlled)
plot_coefs(followers_reg_controlled)
export_summs(followers_reg_controlled)
tidy_followers_reg_controlled <- tidy(followers_reg_controlled)
write.csv(tidy_followers_reg_controlled, "tidy_followers_reg_controlled.csv")
```

```{r followers variation controlled other version, message=FALSE}
library(jtools)
library(broom)
followers_reg_controlled <- lm(bddfinale$v_followers_periode ~ bddfinale$community_other + bddfinale$community_animal_health + bddfinale$community_public_health_france + bddfinale$community_media_public_health_canada + bddfinale$community_proscience_med_france + bddfinale$community_pro_science_france +  bddfinale$community_conspiracy_and_far_right_france + bddfinale$community_conspiracy_and_far_right_international + bddfinale$community_conspiracy_and_far_left + bddfinale$taux_majuscules + bddfinale$taux_antivac + bddfinale$community_mainstream_media +  bddfinale$n_tweets_BDD + bddfinale$active_period + bddfinale$RTpercent + bddfinale$RT_via_percent + bddfinale$taux_tweets_vac + bddfinale$verified + bddfinale$N_mentions + bddfinale$n_followers)
summary(followers_reg_controlled)
plot_summs(followers_reg_controlled)
plot_coefs(followers_reg_controlled)
export_summs(followers_reg_controlled)
tidy_followers_reg_controlled <- tidy(followers_reg_controlled)
write.csv(tidy_followers_reg_controlled, "tidy_followers_reg_controlled.csv")
```


## Analyse de l'évolution des acteurs les plus centraux (indegree) dans le graphe


```{r indegree distribution p1, message=FALSE}
indeg <- degree(network_vax_final_component_w, mode="in")
indeg.dist.p1 <- degree_distribution(network_vax_final_component_w, cumulative=T, mode="in")
plot_degree_distribution <- plot( x=0:max(indeg), y=1-indeg.dist.p1, pch=19, cex=1.2, col="orange",
xlab="In degree", ylab="Cumulative Frequency")
```

On cherche qui ont été les acteurs les plus centraux, et leur communauté:

```{r central p1, message=FALSE}
degree_p1 <- igraph::degree(network_p1, mode = c("in")) 
# get top 10 degrees 
top_p1 <- names(head(sort(degree_p1, decreasing = TRUE), 15)) 
top_p1
write.csv(top_p1, "topcentral_p1.csv")
```


```{r central p2, message=FALSE}
degree_p2 <- igraph::degree(network_p2, mode = c("in")) 
# get top 10 degrees 
top_p2 <- names(head(sort(degree_p2, decreasing = TRUE), 15)) 
top_p2
write.csv(top_p2, "topcentral_p2.csv")
```


```{r central p3, message=FALSE}
degree_p3 <- igraph::degree(network_p3, mode = c("in")) 
# get top 10 degrees 
top_p3 <- names(head(sort(degree_p3, decreasing = TRUE), 15)) 
top_p3
write.csv(top_p3, "topcentral_p3.csv")
```


```{r central p4, message=FALSE}
degree_p4 <- igraph::degree(network_p4, mode = c("in")) 
# get top 10 degrees 
top_p4 <- names(head(sort(degree_p4, decreasing = TRUE), 15)) 
top_p4
write.csv(top_p4, "topcentral_p4.csv")
```


```{r central p5, message=FALSE}
degree_p5 <- igraph::degree(network_p5, mode = c("in")) 
# get top 10 degrees 

top_p5 <- names(head(sort(degree_p5, decreasing = TRUE), 15)) 
top_p5
write.csv(top_p5, "topcentral_p5.csv")
```

## Centralité (in) par communautés

Période 1

```{r degré par communauté, message=FALSE}
centcomp1 <- as.data.frame(degree_p1, network_p1$community)
centcomp1$commmunity_p1 <- network_p1$community
colnames(centcomp1) <- c("value", "variable")
centcomp1 <- filter(centcomp1, centcomp1$variable!= "NA")
```

```{r central p1 par comm, message=FALSE}
library(ggplot2)
library(tidyverse)
library(hrbrthemes)
library(plotly)
library(babynames)
library(viridis)
library(ggthemes)
library(reshape)
box_central_p1 <- ggplot(centcomp1, aes(x = variable, y = value, color = variable, fill=variable)) + geom_boxplot(alpha=0.7) +  stat_summary(fun= mean, geom="point", shape=20, size=3, color="red", fill="red") + ggtitle("2016 - 2nd semester") + coord_flip() + theme_economist() +scale_fill_viridis(discrete=TRUE) + scale_color_viridis(discrete=TRUE)+ xlab("") + scale_y_log10() +
    ylab("Indegree centrality")
box_central_p1 <- box_central_p1 + theme(legend.position = "none")
box_central_p1
```

Période 2

```{r degré par communauté p2, message=FALSE}
centcomp2 <- as.data.frame(degree_p2, network_p2$community)
centcomp2$commmunity_p2 <- network_p2$community
colnames(centcomp2) <- c("value", "variable")
centcomp2 <- filter(centcomp2, centcomp2$variable!= "NA")
```

```{r central p2 par comm, message=FALSE}
box_central_p2 <- ggplot(centcomp2, aes(x = variable, y = value, color = variable, fill=variable)) + geom_boxplot(alpha=0.7) +  stat_summary(fun= mean, geom="point", shape=20, size=3, color="red", fill="red") + ggtitle("2017 - 1st semester") + coord_flip() + theme_economist() +scale_fill_viridis(discrete=TRUE) + scale_color_viridis(discrete=TRUE)+ xlab("") + scale_y_log10() +
    ylab("Indegree centrality")
box_central_p2 <- box_central_p2 + theme(legend.position = "none")
box_central_p2
```

Période 3

```{r degré par communauté p3, message=FALSE}
centcomp3 <- as.data.frame(degree_p3, network_p3$community)
centcomp3$commmunity_p3 <- network_p3$community
colnames(centcomp3) <- c("value", "variable")
centcomp3 <- filter(centcomp3, centcomp3$variable!= "NA")
```

```{r central p3 par comm, message=FALSE}
box_central_p3 <- ggplot(centcomp3, aes(x = variable, y = value, color = variable, fill=variable)) + geom_boxplot(alpha=0.7) +  stat_summary(fun= mean, geom="point", shape=20, size=3, color="red", fill="red") + ggtitle("2017 - 2nd semester") + coord_flip() + theme_economist() +scale_fill_viridis(discrete=TRUE) + scale_color_viridis(discrete=TRUE)+ xlab("") + scale_y_log10() +
    ylab("Indegree centrality")
box_central_p3 <- box_central_p3 + theme(legend.position = "none")
box_central_p3
```
Période 4

```{r degré par communauté p4, message=FALSE}
centcomp4 <- as.data.frame(degree_p4, network_p4$community)
centcomp4$commmunity_p4 <- network_p4$community
colnames(centcomp4) <- c("value", "variable")
centcomp4 <- filter(centcomp4, centcomp4$variable!= "NA")
```


```{r central p4 par comm, message=FALSE}
box_central_p4 <- ggplot(centcomp4, aes(x = variable, y = value, color = variable, fill=variable)) + geom_boxplot(alpha=0.7) +  stat_summary(fun= mean, geom="point", shape=20, size=3, color="red", fill="red") + ggtitle("2018 - 1st semester") + coord_flip() + theme_economist() +scale_fill_viridis(discrete=TRUE) + scale_color_viridis(discrete=TRUE)+ xlab("") + scale_y_log10() +
    ylab("Indegree centrality")
box_central_p4 <- box_central_p4 + theme(legend.position = "none")
box_central_p4
```

Période 5

```{r degré par communauté p5, message=FALSE}
centcomp5 <- as.data.frame(degree_p5, network_p5$community)
centcomp5$commmunity_p5 <- network_p5$community
colnames(centcomp5) <- c("value", "variable")
centcomp5 <- filter(centcomp5, centcomp5$variable!= "NA")
```

```{r central p5 par comm, message=FALSE}
box_central_p5 <- ggplot(centcomp5, aes(x = variable, y = value, color = variable, fill=variable)) + ggtitle("")+ geom_boxplot(alpha=0.7) +  stat_summary(fun= mean, geom="point", shape=20, size=3, color="red", fill="red") + ggtitle("2018 - 2nd semester") + coord_flip() + theme_economist() +scale_fill_viridis(discrete=TRUE) + scale_color_viridis(discrete=TRUE)+ xlab("") + scale_y_log10() +
    ylab("Indegree centrality")
box_central_p5 <- box_central_p5 + theme(legend.position = "none")
box_central_p5
```


## Modification de la centralité au sein des communautés ?

```{r central comm p1, message=FALSE}
top_central_p1 <- data.frame() 
for (i in unique(network_p1$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_p1, v = which(network_p1$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree_p1 <- igraph::degree(subgraph, mode = c("in")) 
    # get top fifteen indegrees 
    top <- names(head(sort(degree_p1, decreasing = TRUE), 5))
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_central_p1 <- top_central_p1 %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_central_p1 %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```


```{r central comm p2, message=FALSE}
top_central_p2 <- data.frame() 
for (i in unique(network_p2$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_p2, v = which(network_p2$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree_p2 <- igraph::degree(subgraph, mode = c("in")) 
    # get top fifteen indegrees 
    top <- names(head(sort(degree_p2, decreasing = TRUE), 5))
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_central_p2 <- top_central_p2 %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_central_p2 %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```

```{r central comm p3, message=FALSE}
top_central_p3 <- data.frame() 
for (i in unique(network_p3$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_p3, v = which(network_p3$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree_p3 <- igraph::degree(subgraph, mode = c("in")) 
    # get top fifteen indegrees 
    top <- names(head(sort(degree_p3, decreasing = TRUE), 5))
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_central_p3 <- top_central_p3 %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_central_p3 %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```

```{r central comm p4, message=FALSE}
top_central_p4 <- data.frame() 
for (i in unique(network_p4$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_p4, v = which(network_p4$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree_p4 <- igraph::degree(subgraph, mode = c("in")) 
    # get top fifteen indegrees 
    top <- names(head(sort(degree_p4, decreasing = TRUE), 5))
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_central_p4 <- top_central_p4 %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_central_p4 %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```

```{r central comm p5, message=FALSE}
top_central_p5 <- data.frame() 
for (i in unique(network_p5$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(network_p5, v = which(network_p5$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 175) { 
    # get degree 
    degree_p5 <- igraph::degree(subgraph, mode = c("in")) 
    # get top fifteen indegrees 
    top <- names(head(sort(degree_p5, decreasing = TRUE), 5))
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_central_p5 <- top_central_p5 %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_central_p5 %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)
```


(Vérifier mobilisation par hashtags par pétition)

## Spécialisation des acteurs

On calcule le taux de tweets parlant de la vaccination à chaque période.

```{r specialisation, message=FALSE}
specialisation_p1 <- sup5_nodes_period1$taux_tweets_vac
specialisation_p2 <- sup5_nodes_period2$taux_tweets_vac
specialisation_p3 <- sup5_nodes_period3$taux_tweets_vac
specialisation_p4 <- sup5_nodes_period4$taux_tweets_vac
specialisation_p5 <- sup5_nodes_period5$taux_tweets_vac
boxplot(specialisation_p1, specialisation_p2, specialisation_p3, specialisation_p4, specialisation_p5, col="#69b3a2", boxwex=0.4 , main="Proportion of tweets about vaccines")
```

Si on refait le calcul sur utilisateurs plus actifs - on visualise en échelle normale puis en log scale.

```{r specialisation actifs}
summary(sup5_nodes_period1$v_tweets_periode)
summary(sup5_nodes_period2$v_tweets_periode)
summary(sup5_nodes_period3$v_tweets_periode)
summary(sup5_nodes_period4$v_tweets_periode)
summary(sup5_nodes_period5$v_tweets_periode)

sup5_nodes_period1_actifs <- sup5_nodes_period1[which(sup5_nodes_period1$v_tweets_periode > 100),]
sup5_nodes_period2_actifs <- sup5_nodes_period2[which(sup5_nodes_period2$v_tweets_periode > 100),]
sup5_nodes_period3_actifs <- sup5_nodes_period3[which(sup5_nodes_period3$v_tweets_periode > 100),]
sup5_nodes_period4_actifs <- sup5_nodes_period4[which(sup5_nodes_period1$v_tweets_periode > 100),]
sup5_nodes_period5_actifs <- sup5_nodes_period5[which(sup5_nodes_period1$v_tweets_periode > 100),]

specialisation_ap1 <- sup5_nodes_period1_actifs$taux_tweets_vac
specialisation_ap2 <- sup5_nodes_period2_actifs$taux_tweets_vac
specialisation_ap3 <- sup5_nodes_period3_actifs$taux_tweets_vac
specialisation_ap4 <- sup5_nodes_period4_actifs$taux_tweets_vac
specialisation_ap5 <- sup5_nodes_period5_actifs$taux_tweets_vac
boxplot(specialisation_ap1, specialisation_ap2, specialisation_ap3, specialisation_ap4, specialisation_ap5, col="#69b3a2", boxwex=0.4 , main="Proportion of tweets about vaccines - users with more than 10 tweets during the period")
boxplot(specialisation_ap1, specialisation_ap2, specialisation_ap3, specialisation_ap4, specialisation_ap5, col="#69b3a2", boxwex=0.4 , main="Proportion of tweets about vaccines - users with more than 10 tweets during the period", log="y")
boxplot(specialisation_ap1, specialisation_ap2, specialisation_ap3, specialisation_ap4, specialisation_ap5, col="#69b3a2", boxwex=0.4 , main="Proportion of tweets about vaccines - users with more than 10 tweets during the period", outline=FALSE)
```

## Activité pendant la période

Quelle variation d'activité pendant la période ?

```{r active p1, message=FALSE}
boxplot(sup5_nodes_period1$v_tweets_periode, sup5_nodes_period2$v_tweets_periode, sup5_nodes_period3$v_tweets_periode, sup5_nodes_period4$v_tweets_periode, sup5_nodes_period5$v_tweets_periode, col="#69b3a2", boxwex=0.4, outline = FALSE)
```

```{r active p1 without outliers, message=FALSE}
boxplot(sup5_nodes_period1$n_tweets_BDD, sup5_nodes_period2$n_tweets_BDD, sup5_nodes_period3$n_tweets_BDD, sup5_nodes_period4$n_tweets_BDD, sup5_nodes_period5$n_tweets_BDD, col="#69b3a2", boxwex=0.4, outline = FALSE, main="Number of tweets regarding vaccines per user during each period")
```

Quelle variation du nombre de maladies citées dans les tweets sur les vaccins ?

```{r nb diseases cited, message=FALSE}
sup5_nodes_period1_maladies <-sup5_nodes_period1[which(sup5_nodes_period1$N_maladies_cites > 0),]
sup5_nodes_period2_maladies <-sup5_nodes_period2[which(sup5_nodes_period1$N_maladies_cites > 0),]
sup5_nodes_period3_maladies <-sup5_nodes_period3[which(sup5_nodes_period1$N_maladies_cites > 0),]
sup5_nodes_period4_maladies <-sup5_nodes_period1[which(sup5_nodes_period1$N_maladies_cites > 0),]
sup5_nodes_period5_maladies <-sup5_nodes_period1[which(sup5_nodes_period1$N_maladies_cites > 0),]

boxplot(sup5_nodes_period1_maladies$N_maladies_cites, sup5_nodes_period2_maladies$N_maladies_cites, sup5_nodes_period3_maladies$N_maladies_cites, sup5_nodes_period4_maladies$N_maladies_cites, sup5_nodes_period5_maladies$N_maladies_cites, main="number of diseases cited in tweets about vaccines by a same user", col="#69b3a2", boxwex=0.4)
```

Quelle variation du nombre de vaccins citées dans les tweets sur les vaccins ?

```{r nb vaccines cited, message=FALSE}
sup5_nodes_period1_vaccins <-sup5_nodes_period1[which(sup5_nodes_period1$N_vaccins_cites > 0),]
sup5_nodes_period2_vaccins <-sup5_nodes_period2[which(sup5_nodes_period1$N_vaccins_cites > 0),]
sup5_nodes_period3_vaccins <-sup5_nodes_period3[which(sup5_nodes_period1$N_vaccins_cites > 0),]
sup5_nodes_period4_vaccins <-sup5_nodes_period1[which(sup5_nodes_period1$N_vaccins_cites > 0),]
sup5_nodes_period5_vaccins <-sup5_nodes_period1[which(sup5_nodes_period1$N_vaccins_cites > 0),]

sizedata1 <- nrow(sup5_nodes_period1_vaccins)
sizedata2 <- nrow(sup5_nodes_period2_vaccins)
sizedata3 <- nrow(sup5_nodes_period3_vaccins)
sizedata4 <- nrow(sup5_nodes_period4_vaccins)
sizedata5 <- nrow(sup5_nodes_period5_vaccins)

boxplot(sup5_nodes_period1_vaccins$N_vaccins_cites, sup5_nodes_period2_vaccins$N_vaccins_cites, sup5_nodes_period3_vaccins$N_vaccins_cites, sup5_nodes_period4_vaccins$N_vaccins_cites, sup5_nodes_period5_vaccins$N_vaccins_cites, main="number of vaccines cited in tweets about vaccines by a same user", col="#69b3a2", boxwex=0.4, width= c(sizedata1,sizedata2,sizedata3, sizedata4, sizedata5))
```


## Restrictions aux militants


```{r militants, results="hide", message=FALSE}
network_p1_m <- delete.vertices(network_p1, which(V(network_p1)$PRO_ANTI_OUT!=c("PRO", "ANTI","OUT")))
V(network_p1_m)$PRO_ANTI_OUT
V(network_p1)$PRO_ANTI_OUT
```

## STERGM

We use Separable Temporal Exponential family Random Graph Models (STERGM) to model the creation and destruction of links across the period. Two equations for each process are estimated.





